{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection de doublons dans les rapports de bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet réalisé en collaboration avec Arthur Mahy(@arthurmahy) et Floriane Ronzon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "En raison de la complexité des systèmes logiciels, les bugs logiciels sont répandus. Les entreprises, en particulier les grandes, utilisent généralement des systèmes de suivi des bugs (BTS), également appelés système de suivi des problèmes, pour gérer et suivre les enregistrements des bugs. Outre les développeurs et les testeurs, de nombreux projets, principalement des projets open source, permettent aux utilisateurs de signaler de nouveaux bugs dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. Un sous-ensemble important de ces champs fournissent des données catégorielles et n'acceptent que les valeurs qui vont d'une liste fixe d’options (par exemple, composant, version et produit du système). Deux autres champs à remplir importants sont le résumé et la description. Les utilisateurs sont libres d'écrire tout ce qui peut décrire le bug au mieux dans les deux champs et la seule contrainte est le nombre de caractères. La soumission d'un formulaire crée une page, appelée rapport de bug ou rapport de problème, qui contient toutes les informations sur un bug.\n",
    "\n",
    "\n",
    "## Objectif \n",
    "En raison du manque de communication et de synchronisation, les utilisateurs peuvent ne pas savoir qu'un bug spécifique a déjà été soumis et le signaler à nouveau. Identifier les rapports de bugs en double est une tâche importante dans les BTS. Fondamentalement, notre objectif est de développer un système qui prédit si une paire de rapports de bug se rapportent au même bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from IPython import display\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de rapport de bug soumis sur la plateforme. Ces rapports sont stockés sous forme de fichiers HTML dans le dossier 'data'. \n",
    "\n",
    "- A : identifiant du bug report\n",
    "- B : date de création\n",
    "- C : résumé\n",
    "- D : produit\n",
    "- E : composant\n",
    "- F : l'identifiant du rapport dont le bug report est dupliqué\n",
    "- G : description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\"bug-report-eclipse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir le chemin du dossier qui contient les données\n",
    "FOLDER_PATH = \"data/\"\n",
    "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
    "\n",
    "# Fichiers d'entraînement et de validation pour les rapports en double\n",
    "training_file = open(os.path.join(FOLDER_PATH, \"training.txt\"))\n",
    "validation_file = open(os.path.join(FOLDER_PATH, \"validation.txt\"))\n",
    "word_vec_path = os.path.join(FOLDER_PATH, \"glove.42B.300d_clear.txt\")\n",
    "\n",
    "\"\"\" \n",
    "Permet la lecture des fichiers sur la liste des rapports\n",
    "\"\"\"\n",
    "def read_dataset(f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        rep1, rep2, label = line.split(',')\n",
    "\n",
    "        rep1 = int(rep1)\n",
    "        rep2 = int(rep2)\n",
    "        label = 1.0 if int(label) > 0 else 0.0 \n",
    "        \n",
    "        yield (rep1, rep2, label)\n",
    "    \n",
    "\n",
    "training_pairs = list(read_dataset(training_file))\n",
    "validation_pairs = list(read_dataset(validation_file))\n",
    "\n",
    "training_reports_set = set()\n",
    "\n",
    "\n",
    "for report1, report2, _ in training_pairs:\n",
    "    training_reports_set.add(report1)\n",
    "    training_reports_set.add(report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "On effectue du web scraping sur les pages HTML des rapports de bugs afin d'obtenir les informations des champs remplis par les utilisateurs. Le web scraping est une extraction des informations d'une page web pour les utiliser dans une analyse calculatoire par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ```extract_data_from_page``` retourne un dictionnaire avec la structure suivante :\n",
    "\n",
    "```python\n",
    " {\"report_id\": int, \n",
    "  \"dup_id\": int or None (identifiant du rapport dont il est dupliqué), \n",
    "  \"component\": str, \n",
    "  \"product\": str, \n",
    "  \"summary\": str, \n",
    "  \"description\": str, \n",
    "  \"creation_date\": str} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    dic = {}\n",
    "    \n",
    "    with open(pagepath) as fp:\n",
    "        soup = BeautifulSoup(fp, \"lxml\")\n",
    "\n",
    "    #Report ID    \n",
    "    dic[\"report_id\"] = int(soup.title.string.split(' ')[0])\n",
    "    \n",
    "    #Duplicate ID\n",
    "    try:\n",
    "        dic[\"dup_id\"] = int(soup.find(id=\"static_bug_status\").a.get('href').split('=')[1])\n",
    "    except:\n",
    "        dic[\"dup_id\"] = None\n",
    "    \n",
    "    #Component\n",
    "    component = soup.find(\"td\", id=\"field_container_component\")\n",
    "    dic[\"component\"] = component.text.replace(\"\\n\", \"\").replace(\"  (show other bugs)\", \"\")\n",
    "\n",
    "    #Product\n",
    "    dic[\"product\"] = soup.find(\"td\", id=\"field_container_product\").text.replace(\"\\n\", \"\")\n",
    "\n",
    "    #Summary\n",
    "    dic[\"summary\"] = soup.find(\"span\", id=\"short_desc_nonedit_display\").text.replace(\"\\n\", \"\")\n",
    "\n",
    "    #Description\n",
    "    dic[\"description\"] = soup.find(\"pre\", class_=\"bz_comment_text\").text\n",
    "        \n",
    "    #Creation date\n",
    "    newtime = soup.find(\"span\", \"bz_comment_time\").next_element.strip().split(':')\n",
    "    dic[\"creation_date\"] = newtime[0] + ':' + newtime[1] + ' ' +(newtime[2])[3:]\n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de texte à partir de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indexer chaque rapport par son identifiant \n",
    "index_path = os.path.join(FOLDER_PATH, 'bug_reports.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    report_index = json.load(open(index_path))\n",
    "    report_index = {int(report_id): data for report_id,data in report_index.items()}\n",
    "else:\n",
    "    # Extraire le contenu d'une page Web \n",
    "\n",
    "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
    "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
    "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
    "\n",
    "    # Enregistrement des données extraites\n",
    "    json.dump(report_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des données\n",
    "\n",
    "Le prétraitement des données est une tache cruciale en fouille de données. Cette étape nettoie et transforme les données brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages, la *tokenization* et le *stemming* sont des étapes cruciales. Il faut également filtrer les mots sans importance pour l'utilisation des algorithmes.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "Par exemple, la phrase \"It's the student's notebook.\" peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce tokenizer remplace la ponctuation par des espaces et tokenise ensuite les tokens qui sont séparés par des espaces blancs (espace, tabulation, nouvelle ligne).\n",
    "def tokenize_space_punk(text):\n",
    "    replace_punctuation = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(replace_punctuation)\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "sentence = \"It's the student's notebook.\"\n",
    "tokens = tokenize_space_punk(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des stopwords\n",
    "\n",
    "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorité des discussions. Les supprimer réduit la dimension du vecteur et accélère les calculs.\n",
    "Les tokens sans importance pour la comparaison des discussions sont ceux qui reviennent dans tous les types de conversations. On utilise un ensemble de stopwords trouvé sur internet (https://www.ranks.nl/stopwords) avec une liste de plus de 600 english stop words. Les supprimer de nos listes de tokens fait donc gagner du temps de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = json.load(open(\"data/stopwords.json\",'r'))\n",
    "\n",
    "# Fonction de filtrage des tokens qui retire les stopwords\n",
    "def filter_tokens(tokens):\n",
    "    return [word_token for word_token in tokens if not word_token in stopwords]\n",
    "\n",
    "filtered_tokens = filter_tokens(tokens)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "La racinisation (stemming) est un procédé de transformation des flexions en leur radical ou racine. Par example, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem).\n",
    "Le stemming permet de gommer les variations morphologiques de mots. Ainsi, on a pu regrouper certains mots qui ont le même sens voire des sens proches (tried et tries par exemple) et diminuer la quantité de vocabulaire utile pour notre modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "word1 = ['I', 'tried', 'different', 'fishes']\n",
    "\n",
    "print([stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
    "print([stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des données\n",
    "\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "De nombreux algorithmes demandent des entrées qui sont toutes de la même taille. Cela n'est pas toujours le cas, notamment pour des données textuelles qui peuvent avoir un nombre variable de mots. \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Monopoly is an awesome game!” La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : \n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "\n",
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au mot *\"games\"* qui apparaît deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "\n",
    "L'utilisation de la fréquence brute d'apparition des mots, comme c'est le cas avec bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de documents de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. À l’inverse, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positifs. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\\begin{equation}\n",
    "\t\\operatorname{IDF}(t) = \\ln\\left( \\frac{N+1}{\\operatorname{df}(t)+1} \\right) + 1,\n",
    "\\end{equation}\n",
    "où $t$ est un token, $N$ est le nombre de documents dans l'ensemble de données, et $\\operatorname{df}(\\cdot)$  est le nombre de documents qui contiennent un mot $i$.\n",
    "\n",
    "Le nouveau poids d'un mot $t$ dans un texte peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw(t) = \\operatorname{tf}(t) \\times \\operatorname{IDF}(t),\n",
    "\\end{equation}\n",
    "où $\\operatorname{tf}(\\cdot)$ est le terme fréquence du mot 𝑖 dans le document 𝑗, c'est-à-dire le nombre de fois qu'un mot apparaît dans le document. *Nous appelons représentation TF-IDF lorsque les poids de la représentation BoW sont calculés au moyen de TF-IDF.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    idf = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Apprend les valeurs IDF basées sur les données textuelles dans X, la matrice des tokens\n",
    "    \"\"\"\n",
    "    def fit(self, X):\n",
    "        self.idf.clear()\n",
    "        for document in X:\n",
    "            document = set(document)\n",
    "            for word in document:\n",
    "                if word in self.idf:\n",
    "                    self.idf[word] += 1\n",
    "                else:\n",
    "                    self.idf[word] = 1\n",
    "                    \n",
    "        N = len(X)\n",
    "        self.idf = {word : np.log((N+1)/(self.idf[word]+1))+1 for word in tqdm.tqdm(list(self.idf.keys()))}\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforme un texte en une représentation TF-IDF\n",
    "    \"\"\"\n",
    "    def transform(self, tokens) :\n",
    "        unique_tokens = list(set(tokens))\n",
    "        \n",
    "        tfidf = {}\n",
    "        for word in unique_tokens:\n",
    "            tfidf[word] = tokens.count(word)\n",
    "        \n",
    "        tfidf = {word : self.idf[word]*tfidf[word] for word in tfidf if word in self.idf}\n",
    "        return list(tfidf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TFIDF class :\")\n",
    "tfidf_test = TFIDF()\n",
    "tfidf_test.fit([['video','awesome', 'The'], ['house', 'The']])\n",
    "tfidf_test.transform(['video','awesome', 'The','The'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "\n",
    "Récemment, un nouveau type de représentation, appelé word embedding ou word vector, s'est révélé très utile pour la PNL. Dans les plongements de mots, les mots sont représentés comme des vecteurs réels, de faible dimension et denses. Ces vecteurs décrivent les positions des mots dans un nouvel espace de caractéristiques qui conservent les informations syntaxiques et sémantiques. Contrairement à d'autres représentations, word embeddings  souffrent moins de la malédiction de la dimensionnalité et améliorent la capacité du modèle à gérer les mots inconnus et rares dans la formation. Par ailleurs,\n",
    "en utilisant word embedding, il est possible d'effectuer des opérations arithmétiques et calculer la distance entre les mots. \n",
    "\n",
    "Dans ce TP, nous utiliserons des incorporations de mots pour générer une représentation dense du texte, appelée *text embedding*.\n",
    "Dans ce contexte, le texte peut contenir une phrase ou plusieurs paragraphes.\n",
    "Le text embedding est calculé comme la moyenne des vecteurs des mots :\n",
    "\\begin{equation}\n",
    "\te_s = \\frac{1}{|s|} \\sum_{t \\in s} w_t,\n",
    "\\end{equation}\n",
    "où $|s|$ est la longueur du texte $s$, $w_t$ est word embedding du token $t$ dans $s$ et $e_s$ est text embedding de $s$.\n",
    "\n",
    "On utilise les vecteurs des mots issus d'un modèle pré-entraîné de *glove.42B.300d_clear.txt* dans le dossier *dataset*.\n",
    "Dans chaque ligne de ce fichier texte, il y a les tokens et leurs valeurs vectorielles. Les valeurs et les tokens sont séparés par des espaces. Dans ce fichier, la longueur de word embedding est de 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding:\n",
    "    def __init__(self):\n",
    "        word_vec_file = open(word_vec_path)\n",
    "        \n",
    "        #Lecture de chaque ligne du fichier pour mettre en forme les vecteurs\n",
    "        word = word_vec_file.readline()\n",
    "        words_vectors = {}\n",
    "        while word != \"\":\n",
    "            word = word.split(' ')\n",
    "            words_vectors[word[0]] = [float(vect) for vect in word[1:]]\n",
    "            word = word_vec_file.readline()\n",
    "        self.words_vectors = words_vectors\n",
    "\n",
    "    \"\"\"\n",
    "    Génère les embeddings d'une liste de tokens\n",
    "    \"\"\"\n",
    "    def generate_embedding(self, tokens):\n",
    "        words_vectors = self.words_vectors\n",
    "        sum = np.zeros(300)\n",
    "\n",
    "        N = 0\n",
    "        for token in tokens:\n",
    "            if token in list(words_vectors.keys()):\n",
    "                sum += np.array(words_vectors[token])\n",
    "                N+=1\n",
    "\n",
    "        if (N != 0):\n",
    "            sum = sum/N\n",
    "            \n",
    "        return list(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TextEmbedding class : \")\n",
    "test = TextEmbedding()\n",
    "print(test.generate_embedding([\"are\",\"better\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Le *pipeline* est la séquence d'étapes de prétraitement des données qui transforme les données brutes dans un format qui permet leur analyse.\n",
    "Le *pipeline* suit les étapes suivantes : \n",
    "- Étape 1. Tokenisation et suppression les mots vides dans le résumé et la description de chaque rapport.\n",
    "- Étape 2. Génération des text embeddings du résumé et de la description pour chaque rapport à partir des textes pré-traités à l'étape précédente.\n",
    "- Étape 3. Application du stemming au tokens obtenus en fin d'étape 1.\n",
    "- Étape 4. Apprentissage de l'IDF en utilisant le résumé et la description de tous les rapports dans le ensemble d'entraînement.\n",
    "- Étape 5. Génération de la représentation TF-IDF du résumé et de la description pour chaque rapport.\n",
    "- Étape 6. Ajout des nouvelles informations calculées au JSON.\n",
    "\n",
    "\n",
    "Après l'exécution du pipeline, chaque rapport dans report_index du fichier JSON quatre clés :\n",
    "     - summary_tfidf : représentation TF-IDF du résumé\n",
    "     - desc_tfidf : représentation TF-IDF de la description\n",
    "     - summary_vec : le text embedding du résumé\n",
    "     - desc_vec : le text embedding  de la description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 1. Tokenisation et suppression les mots vides dans le résumé et la description de chaque rapport.\n",
    "\"\"\"\n",
    "\n",
    "#Dossier du pipeline\n",
    "PIPELINE_PATH = 'data/pipeline'\n",
    "\n",
    "#Chemins des fichiers contenant les summaries et descriptions filtrés\n",
    "filtered_summaries_path = os.path.join(PIPELINE_PATH,'filtered_summaries.json')\n",
    "filtered_descriptions_path = os.path.join(PIPELINE_PATH,'filtered_descriptions.json')\n",
    "\n",
    "if not os.path.exists(PIPELINE_PATH):\n",
    "    os.makedirs(PIPELINE_PATH)\n",
    "\n",
    "#Charger les summaries et descriptions filtrés si déjà existants\n",
    "if os.path.isfile(filtered_summaries_path) and os.path.isfile(filtered_descriptions_path):\n",
    "    filtered_summaries = json.load(open(filtered_summaries_path))\n",
    "    filtered_descriptions = json.load(open(filtered_descriptions_path))\n",
    "\n",
    "#Sinon créer un fichier JSON avec les summaries et descriptions filtrés\n",
    "else:\n",
    "    filtered_summaries = {}\n",
    "    filtered_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(report_index.keys())):\n",
    "        filtered_summaries[report_id] = filter_tokens(tokenize_space_punk((report_index[report_id])[\"summary\"]))\n",
    "        filtered_descriptions[report_id] = filter_tokens(tokenize_space_punk((report_index[report_id])[\"description\"]))\n",
    "    \n",
    "    \n",
    "    json.dump(filtered_summaries, open(filtered_summaries_path,'w'),indent=5)\n",
    "    json.dump(filtered_descriptions, open(filtered_descriptions_path,'w'),indent= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 2. Génération des text embeddings du résumé et de la description pour chaque rapport à partir des textes pré-traités à l'étape précédente.\n",
    "\"\"\"\n",
    "\n",
    "report_index = json.load(open(index_path))\n",
    "report_index = { int(report_id): data for report_id,data in report_index.items()}\n",
    "#Chemins des fichiers contenant les embeddings\n",
    "summaries_embeddings_path = os.path.join(PIPELINE_PATH,'summaries_embeddings.json')\n",
    "descriptions_embeddings_path = os.path.join(PIPELINE_PATH,'descriptions_embeddings.json')\n",
    "\n",
    "\n",
    "#Charger les summaries et descriptions filtrés si déjà existants\n",
    "if os.path.isfile(summaries_embeddings_path) and os.path.isfile(descriptions_embeddings_path):\n",
    "    summaries_embeddings = json.load(open(summaries_embeddings_path))\n",
    "    descriptions_embeddings = json.load(open(descriptions_embeddings_path))\n",
    "\n",
    "else:\n",
    "    summaries_embeddings = {}\n",
    "    descriptions_embeddings = {}\n",
    "    text = TextEmbedding()\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(filtered_summaries.keys())):\n",
    "        summaries_embeddings[report_id] = text.generate_embedding(filtered_summaries[report_id])\n",
    "        descriptions_embeddings[report_id] = text.generate_embedding(filtered_descriptions[report_id])\n",
    "    \n",
    "    #Création JSON des embeddings\n",
    "    json.dump(summaries_embeddings,open(summaries_embeddings_path,'w'))\n",
    "    json.dump(descriptions_embeddings,open(descriptions_embeddings_path,'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 3. Application du stemming au tokens obtenus en fin d'étape 1.\n",
    "\"\"\"\n",
    "\n",
    "#Fichiers des stemmed summaries et stemmed descriptions\n",
    "stemmed_summaries_path = os.path.join(PIPELINE_PATH,'stemmed_summaries.json')\n",
    "stemmed_descriptions_path = os.path.join(PIPELINE_PATH,'stemmed_descriptions.json')\n",
    "\n",
    "#Charger les fichiers si déjà existants\n",
    "if os.path.isfile(stemmed_summaries_path) and os.path.isfile(stemmed_descriptions_path):\n",
    "    stemmed_summaries = json.load(open(stemmed_summaries_path))\n",
    "    stemmed_descriptions = json.load(open(stemmed_descriptions_path))\n",
    "\n",
    "else:\n",
    "    stemmed_summaries = {}\n",
    "    stemmed_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(filtered_summaries.keys())):\n",
    "        stemmed_summaries[report_id] = [stemmer.stem(token) for token in filtered_summaries[report_id]]\n",
    "        stemmed_descriptions[report_id] = [stemmer.stem(token) for token in filtered_descriptions[report_id]]\n",
    "    \n",
    "    #Dump JSON si les fichiers n'existent pas\n",
    "    json.dump(stemmed_summaries,open(stemmed_summaries_path,'w'),indent= 5)\n",
    "    json.dump(stemmed_descriptions,open(stemmed_descriptions_path,'w'),indent= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 4. Apprentissage de l'IDF en utilisant le résumé et la description de tous les rapports dans le ensemble d'entraînement.\n",
    "\"\"\"\n",
    "#Création des listes de tokens d'entrainement\n",
    "training_list_summaries = [stemmed_summaries[report_id] for report_id in list(training_reports_set)]\n",
    "training_list_descriptions = [stemmed_descriptions[report_id] for report_id in list(training_reports_set)]\n",
    "\n",
    "#training_reports_set contient les id de tous les rapports de l'ensemble d'entraînement.\n",
    "training_list = np.unique(training_list_descriptions+training_list_summaries)\n",
    "\n",
    "#Apprentissage de l'IDF\n",
    "tfidf = TFIDF()\n",
    "tfidf.fit(training_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 5. Génération de la représentation TF-IDF du résumé et de la description pour chaque rapport.\n",
    "\"\"\"\n",
    "#Fichiers des tfidf\n",
    "tfidf_summaries_path = os.path.join(PIPELINE_PATH,'tfidf_summaries.json')\n",
    "tfidf_descriptions_path = os.path.join(PIPELINE_PATH,'tfidf_descriptions.json')\n",
    "\n",
    "#Charger les fichiers si déjà existants\n",
    "if os.path.isfile(tfidf_summaries_path) and os.path.isfile(tfidf_descriptions_path):\n",
    "    tfidf_summaries = json.load(open(tfidf_summaries_path))\n",
    "    tfidf_descriptions = json.load(open(tfidf_descriptions_path))\n",
    "\n",
    "else:\n",
    "    tfidf_summaries = {}\n",
    "    tfidf_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(stemmed_summaries.keys())):\n",
    "        tfidf_summaries[report_id] = tfidf.transform(stemmed_summaries[report_id])\n",
    "        tfidf_descriptions[report_id] = tfidf.transform(stemmed_descriptions[report_id])\n",
    "    \n",
    "    #Dump JSON si les fichiers n'existent pas\n",
    "    json.dump(tfidf_summaries,open(tfidf_summaries_path,'w'))\n",
    "    json.dump(tfidf_descriptions,open(tfidf_descriptions_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Étape 6. Ajout des nouvelles informations calculées au JSON.\n",
    "\"\"\"\n",
    "bug_reports_final_path = 'bug_reports_final.json'\n",
    "\n",
    "#On charge le bug_reports_final.json si déjà existant\n",
    "if os.path.isfile(bug_reports_final_path):\n",
    "    report_index = json.load(open(bug_reports_final_path))\n",
    "\n",
    "else:\n",
    "    for report_id in tqdm.tqdm(list(report_index.keys())):\n",
    "        (report_index[report_id])[\"summary_tfidf\"] = tfidf_summaries[str(report_id)]\n",
    "        (report_index[report_id])[\"desc_tfidf\"] = tfidf_descriptions[str(report_id)]\n",
    "        (report_index[report_id])[\"summary_vec\"] = summaries_embeddings[str(report_id)]\n",
    "        (report_index[report_id])[\"desc_vec\"] = descriptions_embeddings[str(report_id)]\n",
    "    \n",
    "    #Dump JSON si le bug_report n'est pas présent\n",
    "    json.dump(report_index,open(\"bug_reports_final.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarité cosinus\n",
    "\n",
    "En traitement du langage naturel, la similarité cosinus est une fonction de similarité populaire utilisée pour comparer les vecteurs de documents. Cette fonction mesure à quel point la direction de deux vecteurs est différente et ses valeurs sont comprises entre -1 et 1.\n",
    "\n",
    "La similarité en cosinus est définie comme :\n",
    "\\begin{equation}\n",
    "    \\operatorname{cos(v, v')} = \\frac{\\sum_{i=1}^{n} v_i  v_i'}{\\sqrt{\\sum_{i=1}^{n} v_i^2} \\sqrt{\\sum_{i=1}^{n}v_i'^2}},\n",
    "\\end{equation}\n",
    "où $v = (v_1, .., v_n)$ et $v' = (v'_1, .., v'_n)$ sont des vecteurs de $n$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calcule la similarité cosinus des TF-IDF de deux rapports r1 et r2\n",
    "\"\"\"\n",
    "def cosine_sim_tf_idf(r1, r2):\n",
    "    dicor1 = {}\n",
    "    dicor2 = {}\n",
    "    \n",
    "    for word, tfidf in r1:\n",
    "        dicor1[word] = tfidf\n",
    "        dicor2[word] = 0\n",
    "    for word, tfidf in r2:\n",
    "        dicor2[word] = tfidf\n",
    "        if dicor1.get(word) is None:\n",
    "            dicor1[word] = 0\n",
    "            \n",
    "    num = 0\n",
    "    denum1 = 0\n",
    "    denum2 = 0\n",
    "    for word in dicor1:\n",
    "        num += (dicor1[word] * dicor2[word])\n",
    "        denum1 += (dicor1[word] * dicor1[word])\n",
    "        denum2 += (dicor2[word] * dicor2[word])\n",
    "        \n",
    "    if denum1 == 0 or denum2 == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return num / (math.sqrt(denum1)*math.sqrt(denum2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TF-IDF cosine similarity : \")\n",
    "report1 = [('A1', 1.4054), ('A2', 1.4054), ('A3', 2.0)]\n",
    "report2 = [ (\"B1\", 1.4054), (\"A2\", 1.4054), (\"A3\", 2.0), (\"A1\", 2.8108)]\n",
    "print(cosine_sim_tf_idf(report1, report2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calcule la similarité cosinus des embeddings de deux rapports\n",
    "\"\"\"\n",
    "def cosine_sim_embedding(vec1, vec2):\n",
    "    num = 0\n",
    "    denum1 = 0\n",
    "    denum2 = 0\n",
    "    for i in range(len(vec1)):\n",
    "        num += (vec1[i] * vec2[i])\n",
    "        denum1 += (vec1[i] * vec1[i])\n",
    "        denum2 += (vec2[i] * vec2[i])\n",
    "    if denum1 == 0 or denum2 == 0 :\n",
    "        return 0\n",
    "    else:  \n",
    "        return num / (math.sqrt(denum1)*math.sqrt(denum2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de features / Feature extraction\n",
    "\n",
    "Nous formons un modèle de régression logistique pour prédire si une paire de rapports se rapportent au même bug ou non. Les features utilisées pour la classification sont énumérées ci-dessous :\n",
    "\n",
    "1. Similitude en cosinus de la représentation TF-IDF des résumés des deux rapports.\n",
    "2. Similitude en cosinus de la représentation TF-IDF des descriptions des deux rapports.\n",
    "3. Similitude en cosinus des embeddings des résumés des deux rapports.\n",
    "4. Similitude en cosinus des embeddings des descriptions des deux rapports.\n",
    "5. Une feature binaire qui est 1.0 lorsque les deux rapports ont le même produit spécifié. Sinon, c'est 0.0.\n",
    "6. Une feature binaire qui est 1.0 lorsque les deux rapports ont le même composant spécifié. Sinon, c'est 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Extraire les features d'une paire de rapports de bugs (r1, r2).\n",
    "    rm_ftr_idxs permet de retirer des features lors de l'apprentissage selon les indexes spécifiés plus haut.\n",
    "    Par exemple, rm_ftr_idxs = [3] permet un apprentissage en utilisant l'ensemble des features excepté la similitude en cosinus des \n",
    "\"\"\"\n",
    "def extract_features(r1, r2, rm_ftr_idxs=[]):\n",
    "    vector = []\n",
    "    if 1 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_tf_idf(r1[\"summary_tfidf\"], r2[\"summary_tfidf\"]))\n",
    "    if 2 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_tf_idf(r1[\"desc_tfidf\"], r2[\"desc_tfidf\"]))\n",
    "    if 3 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_embedding(r1[\"summary_vec\"], r2[\"summary_vec\"]))\n",
    "    if 4 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_embedding(r1[\"desc_vec\"], r2[\"desc_vec\"]))\n",
    "    if 5 not in rm_ftr_idxs:\n",
    "        if r1[\"component\"] == r2[\"component\"]:\n",
    "            vector.append(1.0)\n",
    "        else:\n",
    "            vector.append(0.0)\n",
    "    if 6 not in rm_ftr_idxs:\n",
    "        if r1[\"product\"] == r2[\"product\"]:\n",
    "            vector.append(1.0)\n",
    "        else:\n",
    "            vector.append(0.0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement et test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Charger des étiquettes à partir de l'ensemble d'apprentissage\n",
    "y_train = np.asarray([ y for _, _, y in  training_pairs ])\n",
    "\n",
    "def train_clf(rm_ftr_idxs=[]):\n",
    "    # Extraire les features \n",
    "    X_train = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)], rm_ftr_idxs) for r1, r2, _ in  training_pairs ])\n",
    "    return LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(classifier, X):\n",
    "    # Compute accuracy\n",
    "    y = np.asarray([ y for _, _, y in  validation_pairs ])\n",
    "    return classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mise en forme des données de validation\n",
    "X_test_1 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)]) for r1, r2, _ in validation_pairs])\n",
    "X_test_2 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[1,2]) for r1, r2, _ in validation_pairs])\n",
    "X_test_3 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[3,4]) for r1, r2, _ in validation_pairs])\n",
    "X_test_4 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[5]) for r1, r2, _ in validation_pairs])\n",
    "X_test_5 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[6]) for r1, r2, _ in validation_pairs])\n",
    "\n",
    "#Affichage des précisions\n",
    "print(\"Classifier Accuracy According to the Used Features:\\n\")\n",
    "print(\"1. Classifier with all features\", f\"{compute_acc(train_clf(), X_test_1)*100:.2f} %\")\n",
    "print(\"2. Classifier without TF-IDF cosine similarity:\", f\"{compute_acc(train_clf([1,2]), X_test_2)*100:.2f} %\")\n",
    "print(\"3. Classifier without embedding cosine similarity:\", f\"{compute_acc(train_clf([3,4]), X_test_3)*100:.2f} %\")\n",
    "print(\"4. Classifier without component comparison:\", f\"{compute_acc(train_clf([5]), X_test_4)*100:.2f} %\")\n",
    "print(\"5. Classifier without product comparison:\", f\"{compute_acc(train_clf([6]), X_test_5)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le classificateur avec toutes les caractéristiques offre la meilleure précision (94,75 %), mais la différence de précision entre ce classificateur et ceux sans caractéristiques spécifiques (similitude cosinus TF-IDF, similitude cosinus d'embeddings, comparaison de composants ou comparaison de produits) est relativement faible. L'utilisation du NLP et plus précisément de la méthode bag-of-words permet de détecter les doublons des rapports de bugs dans une très grande majorité des cas. Un compromis entre précision du détecteur et coût de calcul en omettant certaines caractéristiques peut être fait selon les préférences pour le BTS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('FD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a3e1557498e5254914fa9d3d4c811555ae0601fcd06273328e2e97921c0b8766"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
