{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©tection de doublons dans les rapports de bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet r√©alis√© en collaboration avec Arthur Mahy(@arthurmahy) et Floriane Ronzon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "En raison de la complexit√© des syst√®mes logiciels, les bugs logiciels sont r√©pandus. Les entreprises, en particulier les grandes, utilisent g√©n√©ralement des syst√®mes de suivi des bugs (BTS), √©galement appel√©s syst√®me de suivi des probl√®mes, pour g√©rer et suivre les enregistrements des bugs. Outre les d√©veloppeurs et les testeurs, de nombreux projets, principalement des projets open source, permettent aux utilisateurs de signaler de nouveaux bugs dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. Un sous-ensemble important de ces champs fournissent des donn√©es cat√©gorielles et n'acceptent que les valeurs qui vont d'une liste fixe d‚Äôoptions (par exemple, composant, version et produit du syst√®me). Deux autres champs √† remplir importants sont le r√©sum√© et la description. Les utilisateurs sont libres d'√©crire tout ce qui peut d√©crire le bug au mieux dans les deux champs et la seule contrainte est le nombre de caract√®res. La soumission d'un formulaire cr√©e une page, appel√©e rapport de bug ou rapport de probl√®me, qui contient toutes les informations sur un bug.\n",
    "\n",
    "\n",
    "## Objectif \n",
    "En raison du manque de communication et de synchronisation, les utilisateurs peuvent ne pas savoir qu'un bug sp√©cifique a d√©j√† √©t√© soumis et le signaler √† nouveau. Identifier les rapports de bugs en double est une t√¢che importante dans les BTS. Fondamentalement, notre objectif est de d√©velopper un syst√®me qui pr√©dit si une paire de rapports de bug se rapportent au m√™me bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from IPython import display\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import tqdm\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de rapport de bug soumis sur la plateforme. Ces rapports sont stock√©s sous forme de fichiers HTML dans le dossier 'data'. \n",
    "\n",
    "- A : identifiant du bug report\n",
    "- B : date de cr√©ation\n",
    "- C : r√©sum√©\n",
    "- D : produit\n",
    "- E : composant\n",
    "- F : l'identifiant du rapport dont le bug report est dupliqu√©\n",
    "- G : description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\"bug-report-eclipse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finir le chemin du dossier qui contient les donn√©es\n",
    "FOLDER_PATH = \"data/\"\n",
    "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
    "\n",
    "# Fichiers d'entra√Ænement et de validation pour les rapports en double\n",
    "training_file = open(os.path.join(FOLDER_PATH, \"training.txt\"))\n",
    "validation_file = open(os.path.join(FOLDER_PATH, \"validation.txt\"))\n",
    "word_vec_path = os.path.join(FOLDER_PATH, \"glove.42B.300d_clear.txt\")\n",
    "\n",
    "\"\"\" \n",
    "Permet la lecture des fichiers sur la liste des rapports\n",
    "\"\"\"\n",
    "def read_dataset(f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        rep1, rep2, label = line.split(',')\n",
    "\n",
    "        rep1 = int(rep1)\n",
    "        rep2 = int(rep2)\n",
    "        label = 1.0 if int(label) > 0 else 0.0 \n",
    "        \n",
    "        yield (rep1, rep2, label)\n",
    "    \n",
    "\n",
    "training_pairs = list(read_dataset(training_file))\n",
    "validation_pairs = list(read_dataset(validation_file))\n",
    "\n",
    "training_reports_set = set()\n",
    "\n",
    "\n",
    "for report1, report2, _ in training_pairs:\n",
    "    training_reports_set.add(report1)\n",
    "    training_reports_set.add(report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "On effectue du web scraping sur les pages HTML des rapports de bugs afin d'obtenir les informations des champs remplis par les utilisateurs. Le web scraping est une extraction des informations d'une page web pour les utiliser dans une analyse calculatoire par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ```extract_data_from_page``` retourne un dictionnaire avec la structure suivante :\n",
    "\n",
    "```python\n",
    " {\"report_id\": int, \n",
    "  \"dup_id\": int or None (identifiant du rapport dont il est dupliqu√©), \n",
    "  \"component\": str, \n",
    "  \"product\": str, \n",
    "  \"summary\": str, \n",
    "  \"description\": str, \n",
    "  \"creation_date\": str} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    dic = {}\n",
    "    \n",
    "    with open(pagepath) as fp:\n",
    "        soup = BeautifulSoup(fp, \"lxml\")\n",
    "\n",
    "    #Report ID    \n",
    "    dic[\"report_id\"] = int(soup.title.string.split(' ')[0])\n",
    "    \n",
    "    #Duplicate ID\n",
    "    try:\n",
    "        dic[\"dup_id\"] = int(soup.find(id=\"static_bug_status\").a.get('href').split('=')[1])\n",
    "    except:\n",
    "        dic[\"dup_id\"] = None\n",
    "    \n",
    "    #Component\n",
    "    component = soup.find(\"td\", id=\"field_container_component\")\n",
    "    dic[\"component\"] = component.text.replace(\"\\n\", \"\").replace(\"  (show other bugs)\", \"\")\n",
    "\n",
    "    #Product\n",
    "    dic[\"product\"] = soup.find(\"td\", id=\"field_container_product\").text.replace(\"\\n\", \"\")\n",
    "\n",
    "    #Summary\n",
    "    dic[\"summary\"] = soup.find(\"span\", id=\"short_desc_nonedit_display\").text.replace(\"\\n\", \"\")\n",
    "\n",
    "    #Description\n",
    "    dic[\"description\"] = soup.find(\"pre\", class_=\"bz_comment_text\").text\n",
    "        \n",
    "    #Creation date\n",
    "    newtime = soup.find(\"span\", \"bz_comment_time\").next_element.strip().split(':')\n",
    "    dic[\"creation_date\"] = newtime[0] + ':' + newtime[1] + ' ' +(newtime[2])[3:]\n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de texte √† partir de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indexer chaque rapport par son identifiant \n",
    "index_path = os.path.join(FOLDER_PATH, 'bug_reports.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    report_index = json.load(open(index_path))\n",
    "    report_index = {int(report_id): data for report_id,data in report_index.items()}\n",
    "else:\n",
    "    # Extraire le contenu d'une page Web \n",
    "\n",
    "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
    "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
    "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
    "\n",
    "    # Enregistrement des donn√©es extraites\n",
    "    json.dump(report_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©traitement des donn√©es\n",
    "\n",
    "Le pr√©traitement des donn√©es est une tache cruciale en fouille de donn√©es. Cette √©tape nettoie et transforme les donn√©es brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages, la *tokenization* et le *stemming* sont des √©tapes cruciales. Il faut √©galement filtrer les mots sans importance pour l'utilisation des algorithmes.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Cette √©tape permet de s√©parer un texte en s√©quence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "Par exemple, la phrase \"It's the student's notebook.\" peut √™tre s√©par√© en liste de tokens de cette mani√®re: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce tokenizer remplace la ponctuation par des espaces et tokenise ensuite les tokens qui sont s√©par√©s par des espaces blancs (espace, tabulation, nouvelle ligne).\n",
    "def tokenize_space_punk(text):\n",
    "    replace_punctuation = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(replace_punctuation)\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "sentence = \"It's the student's notebook.\"\n",
    "tokens = tokenize_space_punk(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression des stopwords\n",
    "\n",
    "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorit√© des discussions. Les supprimer r√©duit la dimension du vecteur et acc√©l√®re les calculs.\n",
    "Les tokens sans importance pour la comparaison des discussions sont ceux qui reviennent dans tous les types de conversations. On utilise un ensemble de stopwords trouv√© sur internet (https://www.ranks.nl/stopwords) avec une liste de plus de 600 english stop words. Les supprimer de nos listes de tokens fait donc gagner du temps de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = json.load(open(\"data/stopwords.json\",'r'))\n",
    "\n",
    "# Fonction de filtrage des tokens qui retire les stopwords\n",
    "def filter_tokens(tokens):\n",
    "    return [word_token for word_token in tokens if not word_token in stopwords]\n",
    "\n",
    "filtered_tokens = filter_tokens(tokens)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "La racinisation (stemming) est un proc√©d√© de transformation des flexions en leur radical ou racine. Par example, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem).\n",
    "Le stemming permet de gommer les variations morphologiques de mots. Ainsi, on a pu regrouper certains mots qui ont le m√™me sens voire des sens proches (tried et tries par exemple) et diminuer la quantit√© de vocabulaire utile pour notre mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "word1 = ['I', 'tried', 'different', 'fishes']\n",
    "\n",
    "print([stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
    "print([stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repr√©sentation des donn√©es\n",
    "\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "De nombreux algorithmes demandent des entr√©es qui sont toutes de la m√™me taille. Cela n'est pas toujours le cas, notamment pour des donn√©es textuelles qui peuvent avoir un nombre variable de mots. \n",
    "\n",
    "Par exemple, consid√©rons la phrase 1, ‚ÄùBoard games are much better than video games‚Äù et la phrase 2, ‚ÄùMonopoly is an awesome game!‚Äù La table ci-dessous montre un exemple d'un moyen de repr√©sentation de ces deux phrases en utilisant une repr√©sentation fixe : \n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "\n",
    "Chaque colonne repr√©sente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 √† la position (1,7) est due au mot *\"games\"* qui appara√Æt deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne √©tant de longueur 13, on peut les utiliser comme vecteur pour repr√©senter les phrases 1 et 2. Ainsi, c'est cette m√©thode que l'on appelle *Bag-of-Words* : c'est une repr√©sentation de documents par des vecteurs dont la dimension est √©gale √† la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associ√© √† une dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "\n",
    "L'utilisation de la fr√©quence brute d'apparition des mots, comme c'est le cas avec bag-of-words, peut √™tre probl√©matique. En effet, peu de tokens auront une fr√©quence tr√®s √©lev√©e dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance √† biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas √† les discriminer. Par exemple, le mot \"*de*\" appara√Æt dans beaucoup de documents de la base de donn√©es, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. √Ä l‚Äôinverse, le mot \"*g√©nial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'√™tre positifs. TF-IDF est donc une m√©thode qui permet de pallier √† ce probl√®me.\n",
    "\n",
    "TF-IDF pond√®re le vecteur en utilisant une fr√©quence de document inverse (IDF) et une fr√©quence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donn√©, tandis que IDF mesure la capacit√© de discrimination des mots dans un jeu de donn√©es. \n",
    "\n",
    "L'IDF d'un mot se calcule de la fa√ßon suivante:\n",
    "\\begin{equation}\n",
    "\t\\operatorname{IDF}(t) = \\ln\\left( \\frac{N+1}{\\operatorname{df}(t)+1} \\right) + 1,\n",
    "\\end{equation}\n",
    "o√π $t$ est un token, $N$ est le nombre de documents dans l'ensemble de donn√©es, et $\\operatorname{df}(\\cdot)$  est le nombre de documents qui contiennent un mot $i$.\n",
    "\n",
    "Le nouveau poids d'un mot $t$ dans un texte peut ensuite √™tre calcul√© de la fa√ßon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw(t) = \\operatorname{tf}(t) \\times \\operatorname{IDF}(t),\n",
    "\\end{equation}\n",
    "o√π $\\operatorname{tf}(\\cdot)$ est le terme fr√©quence du mot ùëñ dans le document ùëó, c'est-√†-dire le nombre de fois qu'un mot appara√Æt dans le document. *Nous appelons repr√©sentation TF-IDF lorsque les poids de la repr√©sentation BoW sont calcul√©s au moyen de TF-IDF.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    idf = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Apprend les valeurs IDF bas√©es sur les donn√©es textuelles dans X, la matrice des tokens\n",
    "    \"\"\"\n",
    "    def fit(self, X):\n",
    "        self.idf.clear()\n",
    "        for document in X:\n",
    "            document = set(document)\n",
    "            for word in document:\n",
    "                if word in self.idf:\n",
    "                    self.idf[word] += 1\n",
    "                else:\n",
    "                    self.idf[word] = 1\n",
    "                    \n",
    "        N = len(X)\n",
    "        self.idf = {word : np.log((N+1)/(self.idf[word]+1))+1 for word in tqdm.tqdm(list(self.idf.keys()))}\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforme un texte en une repr√©sentation TF-IDF\n",
    "    \"\"\"\n",
    "    def transform(self, tokens) :\n",
    "        unique_tokens = list(set(tokens))\n",
    "        \n",
    "        tfidf = {}\n",
    "        for word in unique_tokens:\n",
    "            tfidf[word] = tokens.count(word)\n",
    "        \n",
    "        tfidf = {word : self.idf[word]*tfidf[word] for word in tfidf if word in self.idf}\n",
    "        return list(tfidf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TFIDF class :\")\n",
    "tfidf_test = TFIDF()\n",
    "tfidf_test.fit([['video','awesome', 'The'], ['house', 'The']])\n",
    "tfidf_test.transform(['video','awesome', 'The','The'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "\n",
    "R√©cemment, un nouveau type de repr√©sentation, appel√© word embedding ou word vector, s'est r√©v√©l√© tr√®s utile pour la PNL. Dans les plongements de mots, les mots sont repr√©sent√©s comme des vecteurs r√©els, de faible dimension et denses. Ces vecteurs d√©crivent les positions des mots dans un nouvel espace de caract√©ristiques qui conservent les informations syntaxiques et s√©mantiques. Contrairement √† d'autres repr√©sentations, word embeddings  souffrent moins de la mal√©diction de la dimensionnalit√© et am√©liorent la capacit√© du mod√®le √† g√©rer les mots inconnus et rares dans la formation. Par ailleurs,\n",
    "en utilisant word embedding, il est possible d'effectuer des op√©rations arithm√©tiques et calculer la distance entre les mots. \n",
    "\n",
    "Dans ce TP, nous utiliserons des incorporations de mots pour g√©n√©rer une repr√©sentation dense du texte, appel√©e *text embedding*.\n",
    "Dans ce contexte, le texte peut contenir une phrase ou plusieurs paragraphes.\n",
    "Le text embedding est calcul√© comme la moyenne des vecteurs des mots :\n",
    "\\begin{equation}\n",
    "\te_s = \\frac{1}{|s|} \\sum_{t \\in s} w_t,\n",
    "\\end{equation}\n",
    "o√π $|s|$ est la longueur du texte $s$, $w_t$ est word embedding du token $t$ dans $s$ et $e_s$ est text embedding de $s$.\n",
    "\n",
    "On utilise les vecteurs des mots issus d'un mod√®le pr√©-entra√Æn√© de *glove.42B.300d_clear.txt* dans le dossier *dataset*.\n",
    "Dans chaque ligne de ce fichier texte, il y a les tokens et leurs valeurs vectorielles. Les valeurs et les tokens sont s√©par√©s par des espaces. Dans ce fichier, la longueur de word embedding est de 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding:\n",
    "    def __init__(self):\n",
    "        word_vec_file = open(word_vec_path)\n",
    "        \n",
    "        #Lecture de chaque ligne du fichier pour mettre en forme les vecteurs\n",
    "        word = word_vec_file.readline()\n",
    "        words_vectors = {}\n",
    "        while word != \"\":\n",
    "            word = word.split(' ')\n",
    "            words_vectors[word[0]] = [float(vect) for vect in word[1:]]\n",
    "            word = word_vec_file.readline()\n",
    "        self.words_vectors = words_vectors\n",
    "\n",
    "    \"\"\"\n",
    "    G√©n√®re les embeddings d'une liste de tokens\n",
    "    \"\"\"\n",
    "    def generate_embedding(self, tokens):\n",
    "        words_vectors = self.words_vectors\n",
    "        sum = np.zeros(300)\n",
    "\n",
    "        N = 0\n",
    "        for token in tokens:\n",
    "            if token in list(words_vectors.keys()):\n",
    "                sum += np.array(words_vectors[token])\n",
    "                N+=1\n",
    "\n",
    "        if (N != 0):\n",
    "            sum = sum/N\n",
    "            \n",
    "        return list(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TextEmbedding class : \")\n",
    "test = TextEmbedding()\n",
    "print(test.generate_embedding([\"are\",\"better\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Le *pipeline* est la s√©quence d'√©tapes de pr√©traitement des donn√©es qui transforme les donn√©es brutes dans un format qui permet leur analyse.\n",
    "Le *pipeline* suit les √©tapes suivantes : \n",
    "- √âtape 1. Tokenisation et suppression les mots vides dans le r√©sum√© et la description de chaque rapport.\n",
    "- √âtape 2. G√©n√©ration des text embeddings du r√©sum√© et de la description pour chaque rapport √† partir des textes pr√©-trait√©s √† l'√©tape pr√©c√©dente.\n",
    "- √âtape 3. Application du stemming au tokens obtenus en fin d'√©tape 1.\n",
    "- √âtape 4. Apprentissage de l'IDF en utilisant le r√©sum√© et la description de tous les rapports dans le ensemble d'entra√Ænement.\n",
    "- √âtape 5. G√©n√©ration de la repr√©sentation TF-IDF du r√©sum√© et de la description pour chaque rapport.\n",
    "- √âtape 6. Ajout des nouvelles informations calcul√©es au JSON.\n",
    "\n",
    "\n",
    "Apr√®s l'ex√©cution du pipeline, chaque rapport dans report_index du fichier JSON quatre cl√©s :\n",
    "     - summary_tfidf : repr√©sentation TF-IDF du r√©sum√©\n",
    "     - desc_tfidf : repr√©sentation TF-IDF de la description\n",
    "     - summary_vec : le text embedding du r√©sum√©\n",
    "     - desc_vec : le text embedding  de la description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 1. Tokenisation et suppression les mots vides dans le r√©sum√© et la description de chaque rapport.\n",
    "\"\"\"\n",
    "\n",
    "#Dossier du pipeline\n",
    "PIPELINE_PATH = 'data/pipeline'\n",
    "\n",
    "#Chemins des fichiers contenant les summaries et descriptions filtr√©s\n",
    "filtered_summaries_path = os.path.join(PIPELINE_PATH,'filtered_summaries.json')\n",
    "filtered_descriptions_path = os.path.join(PIPELINE_PATH,'filtered_descriptions.json')\n",
    "\n",
    "if not os.path.exists(PIPELINE_PATH):\n",
    "    os.makedirs(PIPELINE_PATH)\n",
    "\n",
    "#Charger les summaries et descriptions filtr√©s si d√©j√† existants\n",
    "if os.path.isfile(filtered_summaries_path) and os.path.isfile(filtered_descriptions_path):\n",
    "    filtered_summaries = json.load(open(filtered_summaries_path))\n",
    "    filtered_descriptions = json.load(open(filtered_descriptions_path))\n",
    "\n",
    "#Sinon cr√©er un fichier JSON avec les summaries et descriptions filtr√©s\n",
    "else:\n",
    "    filtered_summaries = {}\n",
    "    filtered_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(report_index.keys())):\n",
    "        filtered_summaries[report_id] = filter_tokens(tokenize_space_punk((report_index[report_id])[\"summary\"]))\n",
    "        filtered_descriptions[report_id] = filter_tokens(tokenize_space_punk((report_index[report_id])[\"description\"]))\n",
    "    \n",
    "    \n",
    "    json.dump(filtered_summaries, open(filtered_summaries_path,'w'),indent=5)\n",
    "    json.dump(filtered_descriptions, open(filtered_descriptions_path,'w'),indent= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 2. G√©n√©ration des text embeddings du r√©sum√© et de la description pour chaque rapport √† partir des textes pr√©-trait√©s √† l'√©tape pr√©c√©dente.\n",
    "\"\"\"\n",
    "\n",
    "report_index = json.load(open(index_path))\n",
    "report_index = { int(report_id): data for report_id,data in report_index.items()}\n",
    "#Chemins des fichiers contenant les embeddings\n",
    "summaries_embeddings_path = os.path.join(PIPELINE_PATH,'summaries_embeddings.json')\n",
    "descriptions_embeddings_path = os.path.join(PIPELINE_PATH,'descriptions_embeddings.json')\n",
    "\n",
    "\n",
    "#Charger les summaries et descriptions filtr√©s si d√©j√† existants\n",
    "if os.path.isfile(summaries_embeddings_path) and os.path.isfile(descriptions_embeddings_path):\n",
    "    summaries_embeddings = json.load(open(summaries_embeddings_path))\n",
    "    descriptions_embeddings = json.load(open(descriptions_embeddings_path))\n",
    "\n",
    "else:\n",
    "    summaries_embeddings = {}\n",
    "    descriptions_embeddings = {}\n",
    "    text = TextEmbedding()\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(filtered_summaries.keys())):\n",
    "        summaries_embeddings[report_id] = text.generate_embedding(filtered_summaries[report_id])\n",
    "        descriptions_embeddings[report_id] = text.generate_embedding(filtered_descriptions[report_id])\n",
    "    \n",
    "    #Cr√©ation JSON des embeddings\n",
    "    json.dump(summaries_embeddings,open(summaries_embeddings_path,'w'))\n",
    "    json.dump(descriptions_embeddings,open(descriptions_embeddings_path,'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 3. Application du stemming au tokens obtenus en fin d'√©tape 1.\n",
    "\"\"\"\n",
    "\n",
    "#Fichiers des stemmed summaries et stemmed descriptions\n",
    "stemmed_summaries_path = os.path.join(PIPELINE_PATH,'stemmed_summaries.json')\n",
    "stemmed_descriptions_path = os.path.join(PIPELINE_PATH,'stemmed_descriptions.json')\n",
    "\n",
    "#Charger les fichiers si d√©j√† existants\n",
    "if os.path.isfile(stemmed_summaries_path) and os.path.isfile(stemmed_descriptions_path):\n",
    "    stemmed_summaries = json.load(open(stemmed_summaries_path))\n",
    "    stemmed_descriptions = json.load(open(stemmed_descriptions_path))\n",
    "\n",
    "else:\n",
    "    stemmed_summaries = {}\n",
    "    stemmed_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(filtered_summaries.keys())):\n",
    "        stemmed_summaries[report_id] = [stemmer.stem(token) for token in filtered_summaries[report_id]]\n",
    "        stemmed_descriptions[report_id] = [stemmer.stem(token) for token in filtered_descriptions[report_id]]\n",
    "    \n",
    "    #Dump JSON si les fichiers n'existent pas\n",
    "    json.dump(stemmed_summaries,open(stemmed_summaries_path,'w'),indent= 5)\n",
    "    json.dump(stemmed_descriptions,open(stemmed_descriptions_path,'w'),indent= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 4. Apprentissage de l'IDF en utilisant le r√©sum√© et la description de tous les rapports dans le ensemble d'entra√Ænement.\n",
    "\"\"\"\n",
    "#Cr√©ation des listes de tokens d'entrainement\n",
    "training_list_summaries = [stemmed_summaries[report_id] for report_id in list(training_reports_set)]\n",
    "training_list_descriptions = [stemmed_descriptions[report_id] for report_id in list(training_reports_set)]\n",
    "\n",
    "#training_reports_set contient les id de tous les rapports de l'ensemble d'entra√Ænement.\n",
    "training_list = np.unique(training_list_descriptions+training_list_summaries)\n",
    "\n",
    "#Apprentissage de l'IDF\n",
    "tfidf = TFIDF()\n",
    "tfidf.fit(training_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 5. G√©n√©ration de la repr√©sentation TF-IDF du r√©sum√© et de la description pour chaque rapport.\n",
    "\"\"\"\n",
    "#Fichiers des tfidf\n",
    "tfidf_summaries_path = os.path.join(PIPELINE_PATH,'tfidf_summaries.json')\n",
    "tfidf_descriptions_path = os.path.join(PIPELINE_PATH,'tfidf_descriptions.json')\n",
    "\n",
    "#Charger les fichiers si d√©j√† existants\n",
    "if os.path.isfile(tfidf_summaries_path) and os.path.isfile(tfidf_descriptions_path):\n",
    "    tfidf_summaries = json.load(open(tfidf_summaries_path))\n",
    "    tfidf_descriptions = json.load(open(tfidf_descriptions_path))\n",
    "\n",
    "else:\n",
    "    tfidf_summaries = {}\n",
    "    tfidf_descriptions = {}\n",
    "\n",
    "    for report_id in tqdm.tqdm(list(stemmed_summaries.keys())):\n",
    "        tfidf_summaries[report_id] = tfidf.transform(stemmed_summaries[report_id])\n",
    "        tfidf_descriptions[report_id] = tfidf.transform(stemmed_descriptions[report_id])\n",
    "    \n",
    "    #Dump JSON si les fichiers n'existent pas\n",
    "    json.dump(tfidf_summaries,open(tfidf_summaries_path,'w'))\n",
    "    json.dump(tfidf_descriptions,open(tfidf_descriptions_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 6. Ajout des nouvelles informations calcul√©es au JSON.\n",
    "\"\"\"\n",
    "bug_reports_final_path = 'bug_reports_final.json'\n",
    "\n",
    "#On charge le bug_reports_final.json si d√©j√† existant\n",
    "if os.path.isfile(bug_reports_final_path):\n",
    "    report_index = json.load(open(bug_reports_final_path))\n",
    "\n",
    "else:\n",
    "    for report_id in tqdm.tqdm(list(report_index.keys())):\n",
    "        (report_index[report_id])[\"summary_tfidf\"] = tfidf_summaries[str(report_id)]\n",
    "        (report_index[report_id])[\"desc_tfidf\"] = tfidf_descriptions[str(report_id)]\n",
    "        (report_index[report_id])[\"summary_vec\"] = summaries_embeddings[str(report_id)]\n",
    "        (report_index[report_id])[\"desc_vec\"] = descriptions_embeddings[str(report_id)]\n",
    "    \n",
    "    #Dump JSON si le bug_report n'est pas pr√©sent\n",
    "    json.dump(report_index,open(\"bug_reports_final.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√© cosinus\n",
    "\n",
    "En traitement du langage naturel, la similarit√© cosinus est une fonction de similarit√© populaire utilis√©e pour comparer les vecteurs de documents. Cette fonction mesure √† quel point la direction de deux vecteurs est diff√©rente et ses valeurs sont comprises entre -1 et 1.\n",
    "\n",
    "La similarit√© en cosinus est d√©finie comme :\n",
    "\\begin{equation}\n",
    "    \\operatorname{cos(v, v')} = \\frac{\\sum_{i=1}^{n} v_i  v_i'}{\\sqrt{\\sum_{i=1}^{n} v_i^2} \\sqrt{\\sum_{i=1}^{n}v_i'^2}},\n",
    "\\end{equation}\n",
    "o√π $v = (v_1, .., v_n)$ et $v' = (v'_1, .., v'_n)$ sont des vecteurs de $n$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calcule la similarit√© cosinus des TF-IDF de deux rapports r1 et r2\n",
    "\"\"\"\n",
    "def cosine_sim_tf_idf(r1, r2):\n",
    "    dicor1 = {}\n",
    "    dicor2 = {}\n",
    "    \n",
    "    for word, tfidf in r1:\n",
    "        dicor1[word] = tfidf\n",
    "        dicor2[word] = 0\n",
    "    for word, tfidf in r2:\n",
    "        dicor2[word] = tfidf\n",
    "        if dicor1.get(word) is None:\n",
    "            dicor1[word] = 0\n",
    "            \n",
    "    num = 0\n",
    "    denum1 = 0\n",
    "    denum2 = 0\n",
    "    for word in dicor1:\n",
    "        num += (dicor1[word] * dicor2[word])\n",
    "        denum1 += (dicor1[word] * dicor1[word])\n",
    "        denum2 += (dicor2[word] * dicor2[word])\n",
    "        \n",
    "    if denum1 == 0 or denum2 == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return num / (math.sqrt(denum1)*math.sqrt(denum2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing TF-IDF cosine similarity : \")\n",
    "report1 = [('A1', 1.4054), ('A2', 1.4054), ('A3', 2.0)]\n",
    "report2 = [ (\"B1\", 1.4054), (\"A2\", 1.4054), (\"A3\", 2.0), (\"A1\", 2.8108)]\n",
    "print(cosine_sim_tf_idf(report1, report2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calcule la similarit√© cosinus des embeddings de deux rapports\n",
    "\"\"\"\n",
    "def cosine_sim_embedding(vec1, vec2):\n",
    "    num = 0\n",
    "    denum1 = 0\n",
    "    denum2 = 0\n",
    "    for i in range(len(vec1)):\n",
    "        num += (vec1[i] * vec2[i])\n",
    "        denum1 += (vec1[i] * vec1[i])\n",
    "        denum2 += (vec2[i] * vec2[i])\n",
    "    if denum1 == 0 or denum2 == 0 :\n",
    "        return 0\n",
    "    else:  \n",
    "        return num / (math.sqrt(denum1)*math.sqrt(denum2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de features / Feature extraction\n",
    "\n",
    "Nous formons un mod√®le de r√©gression logistique pour pr√©dire si une paire de rapports se rapportent au m√™me bug ou non. Les features utilis√©es pour la classification sont √©num√©r√©es ci-dessous :\n",
    "\n",
    "1. Similitude en cosinus de la repr√©sentation TF-IDF des r√©sum√©s des deux rapports.\n",
    "2. Similitude en cosinus de la repr√©sentation TF-IDF des descriptions des deux rapports.\n",
    "3. Similitude en cosinus des embeddings des r√©sum√©s des deux rapports.\n",
    "4. Similitude en cosinus des embeddings des descriptions des deux rapports.\n",
    "5. Une feature binaire qui est 1.0 lorsque les deux rapports ont le m√™me produit sp√©cifi√©. Sinon, c'est 0.0.\n",
    "6. Une feature binaire qui est 1.0 lorsque les deux rapports ont le m√™me composant sp√©cifi√©. Sinon, c'est 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Extraire les features d'une paire de rapports de bugs (r1, r2).\n",
    "    rm_ftr_idxs permet de retirer des features lors de l'apprentissage selon les indexes sp√©cifi√©s plus haut.\n",
    "    Par exemple, rm_ftr_idxs = [3] permet un apprentissage en utilisant l'ensemble des features except√© la similitude en cosinus des \n",
    "\"\"\"\n",
    "def extract_features(r1, r2, rm_ftr_idxs=[]):\n",
    "    vector = []\n",
    "    if 1 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_tf_idf(r1[\"summary_tfidf\"], r2[\"summary_tfidf\"]))\n",
    "    if 2 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_tf_idf(r1[\"desc_tfidf\"], r2[\"desc_tfidf\"]))\n",
    "    if 3 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_embedding(r1[\"summary_vec\"], r2[\"summary_vec\"]))\n",
    "    if 4 not in rm_ftr_idxs:\n",
    "        vector.append(cosine_sim_embedding(r1[\"desc_vec\"], r2[\"desc_vec\"]))\n",
    "    if 5 not in rm_ftr_idxs:\n",
    "        if r1[\"component\"] == r2[\"component\"]:\n",
    "            vector.append(1.0)\n",
    "        else:\n",
    "            vector.append(0.0)\n",
    "    if 6 not in rm_ftr_idxs:\n",
    "        if r1[\"product\"] == r2[\"product\"]:\n",
    "            vector.append(1.0)\n",
    "        else:\n",
    "            vector.append(0.0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Ænement et test du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Charger des √©tiquettes √† partir de l'ensemble d'apprentissage\n",
    "y_train = np.asarray([ y for _, _, y in  training_pairs ])\n",
    "\n",
    "def train_clf(rm_ftr_idxs=[]):\n",
    "    # Extraire les features \n",
    "    X_train = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)], rm_ftr_idxs) for r1, r2, _ in  training_pairs ])\n",
    "    return LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(classifier, X):\n",
    "    # Compute accuracy\n",
    "    y = np.asarray([ y for _, _, y in  validation_pairs ])\n",
    "    return classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mise en forme des donn√©es de validation\n",
    "X_test_1 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)]) for r1, r2, _ in validation_pairs])\n",
    "X_test_2 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[1,2]) for r1, r2, _ in validation_pairs])\n",
    "X_test_3 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[3,4]) for r1, r2, _ in validation_pairs])\n",
    "X_test_4 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[5]) for r1, r2, _ in validation_pairs])\n",
    "X_test_5 = np.asarray([extract_features(report_index[str(r1)], report_index[str(r2)],[6]) for r1, r2, _ in validation_pairs])\n",
    "\n",
    "#Affichage des pr√©cisions\n",
    "print(\"Classifier Accuracy According to the Used Features:\\n\")\n",
    "print(\"1. Classifier with all features\", f\"{compute_acc(train_clf(), X_test_1)*100:.2f} %\")\n",
    "print(\"2. Classifier without TF-IDF cosine similarity:\", f\"{compute_acc(train_clf([1,2]), X_test_2)*100:.2f} %\")\n",
    "print(\"3. Classifier without embedding cosine similarity:\", f\"{compute_acc(train_clf([3,4]), X_test_3)*100:.2f} %\")\n",
    "print(\"4. Classifier without component comparison:\", f\"{compute_acc(train_clf([5]), X_test_4)*100:.2f} %\")\n",
    "print(\"5. Classifier without product comparison:\", f\"{compute_acc(train_clf([6]), X_test_5)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le classificateur avec toutes les caract√©ristiques offre la meilleure pr√©cision (94,75 %), mais la diff√©rence de pr√©cision entre ce classificateur et ceux sans caract√©ristiques sp√©cifiques (similitude cosinus TF-IDF, similitude cosinus d'embeddings, comparaison de composants ou comparaison de produits) est relativement faible. L'utilisation du NLP et plus pr√©cis√©ment de la m√©thode bag-of-words permet de d√©tecter les doublons des rapports de bugs dans une tr√®s grande majorit√© des cas. Un compromis entre pr√©cision du d√©tecteur et co√ªt de calcul en omettant certaines caract√©ristiques peut √™tre fait selon les pr√©f√©rences pour le BTS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('FD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a3e1557498e5254914fa9d3d4c811555ae0601fcd06273328e2e97921c0b8766"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
